{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "\n",
    "---\n",
    "### 1. Import the libraries that we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing import image\n",
    "from keras.applications import imagenet_utils\n",
    "from keras.applications import vgg16\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.metrics import categorical_crossentropy\n",
    "\n",
    "from keras.layers import Dense, Flatten, Dropout, BatchNormalization\n",
    "from keras.models import Model\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path  = 'data/train'\n",
    "valid_path  = 'data/valid'\n",
    "test_path  = 'data/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400 images belonging to 2 classes.\n",
      "Found 200 images belonging to 2 classes.\n",
      "Found 200 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# ImageDataGenerator generates batches of tensor image data with real-time data augmentation. \n",
    "# The data will be looped over (in batches).\n",
    "# in this example, we won't be doing any image augmentation\n",
    "train_batches = ImageDataGenerator().flow_from_directory(train_path, \n",
    "                                                         target_size=(224,224), \n",
    "                                                         batch_size=10)\n",
    "\n",
    "valid_batches = ImageDataGenerator().flow_from_directory(valid_path,\n",
    "                                                         target_size=(224,224), \n",
    "                                                         batch_size=30)\n",
    "\n",
    "test_batches = ImageDataGenerator().flow_from_directory(test_path, \n",
    "                                                        target_size=(224,224), \n",
    "                                                        batch_size=50, \n",
    "                                                        shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. VGG16 base model pre-trained on ImageNet dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = vgg16.VGG16(weights = \"imagenet\", include_top=False, input_shape = (224,224, 3), pooling='avg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. freeze the classification layers in the base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 512)               0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 7,079,424\n",
      "Non-trainable params: 7,635,264\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# iterate through its layers and lock them to make them not trainable with this code\n",
    "for layer in base_model.layers[:-5]:\n",
    "    layer.trainable = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "softmax (Dense)              (None, 2)                 1026      \n",
      "=================================================================\n",
      "Total params: 14,715,714\n",
      "Trainable params: 7,080,450\n",
      "Non-trainable params: 7,635,264\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# save the output of the base_model to be the input of the next layer\n",
    "last_output = base_model.output\n",
    " \n",
    "# add our new softmax layer with 10 hidden units\n",
    "x = Dense(2, activation='softmax', name='softmax')(last_output)\n",
    " \n",
    "# instantiate a new_model using keras’s Model class\n",
    "new_model = Model(inputs=base_model.input, outputs=x)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train the new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.compile(Adam(lr=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-9-522af335f445>:5: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "Epoch 1/20\n",
      "18/18 [==============================] - 41s 2s/step - loss: 0.8865 - accuracy: 0.6333 - val_loss: 0.4158 - val_accuracy: 0.8000\n",
      "Epoch 2/20\n",
      "18/18 [==============================] - 42s 2s/step - loss: 0.4067 - accuracy: 0.8056 - val_loss: 0.3910 - val_accuracy: 0.8444\n",
      "Epoch 3/20\n",
      "18/18 [==============================] - 45s 2s/step - loss: 0.4743 - accuracy: 0.7500 - val_loss: 0.3781 - val_accuracy: 0.8333\n",
      "Epoch 4/20\n",
      "18/18 [==============================] - 43s 2s/step - loss: 0.3909 - accuracy: 0.8056 - val_loss: 0.3840 - val_accuracy: 0.8333\n",
      "Epoch 5/20\n",
      "18/18 [==============================] - 45s 3s/step - loss: 0.4201 - accuracy: 0.7833 - val_loss: 0.4183 - val_accuracy: 0.8333\n",
      "Epoch 6/20\n",
      "18/18 [==============================] - 48s 3s/step - loss: 0.3999 - accuracy: 0.7944 - val_loss: 0.5130 - val_accuracy: 0.7778\n",
      "Epoch 7/20\n",
      "18/18 [==============================] - 43s 2s/step - loss: 0.3698 - accuracy: 0.8444 - val_loss: 0.3438 - val_accuracy: 0.8333\n",
      "Epoch 8/20\n",
      "18/18 [==============================] - 46s 3s/step - loss: 0.3394 - accuracy: 0.8556 - val_loss: 0.4210 - val_accuracy: 0.7889\n",
      "Epoch 9/20\n",
      "18/18 [==============================] - 49s 3s/step - loss: 0.3261 - accuracy: 0.8611 - val_loss: 0.4279 - val_accuracy: 0.7778\n",
      "Epoch 10/20\n",
      "18/18 [==============================] - 47s 3s/step - loss: 0.2568 - accuracy: 0.9000 - val_loss: 0.4121 - val_accuracy: 0.8333\n",
      "Epoch 11/20\n",
      "18/18 [==============================] - 46s 3s/step - loss: 0.2875 - accuracy: 0.8722 - val_loss: 0.4274 - val_accuracy: 0.7889\n",
      "Epoch 12/20\n",
      "18/18 [==============================] - 47s 3s/step - loss: 0.2511 - accuracy: 0.9056 - val_loss: 0.4127 - val_accuracy: 0.8111\n",
      "Epoch 13/20\n",
      "18/18 [==============================] - 43s 2s/step - loss: 0.1937 - accuracy: 0.9333 - val_loss: 0.4257 - val_accuracy: 0.8222\n",
      "Epoch 14/20\n",
      "18/18 [==============================] - 43s 2s/step - loss: 0.1203 - accuracy: 0.9722 - val_loss: 0.6920 - val_accuracy: 0.7778\n",
      "Epoch 15/20\n",
      "18/18 [==============================] - 43s 2s/step - loss: 0.1715 - accuracy: 0.9389 - val_loss: 0.5371 - val_accuracy: 0.7333\n",
      "Epoch 16/20\n",
      "18/18 [==============================] - 43s 2s/step - loss: 0.1428 - accuracy: 0.9278 - val_loss: 0.4526 - val_accuracy: 0.7778\n",
      "Epoch 17/20\n",
      "18/18 [==============================] - 43s 2s/step - loss: 0.1590 - accuracy: 0.9278 - val_loss: 0.5391 - val_accuracy: 0.8000\n",
      "Epoch 18/20\n",
      "18/18 [==============================] - 43s 2s/step - loss: 0.1667 - accuracy: 0.9333 - val_loss: 0.2310 - val_accuracy: 0.8778\n",
      "Epoch 19/20\n",
      "18/18 [==============================] - 43s 2s/step - loss: 0.1411 - accuracy: 0.9444 - val_loss: 0.3987 - val_accuracy: 0.8444\n",
      "Epoch 20/20\n",
      "18/18 [==============================] - 43s 2s/step - loss: 0.1479 - accuracy: 0.9444 - val_loss: 0.7035 - val_accuracy: 0.7778\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='signlanguage.model.hdf5', save_best_only=True)\n",
    "\n",
    "history = new_model.fit_generator(train_batches, steps_per_epoch=18,\n",
    "                   validation_data=valid_batches, validation_steps=3, epochs=20, verbose=1, callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. create the confusion matrix to evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path):\n",
    "    data = load_files(path)\n",
    "    paths = np.array(data['filenames'])\n",
    "    targets = np_utils.to_categorical(np.array(data['target']))\n",
    "    return paths, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_files\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "\n",
    "test_files, test_targets = load_dataset('data/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "  0%|                                                  | 0/200 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████| 200/200 [00:00<00:00, 1086.78it/s]\u001b[A\u001b[A\u001b[A\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import image  \n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from tqdm import tqdm\n",
    "\n",
    "def path_to_tensor(img_path):\n",
    "    # loads RGB image as PIL.Image.Image type\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    # convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)\n",
    "    x = image.img_to_array(img)\n",
    "    # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor\n",
    "    return np.expand_dims(x, axis=0)\n",
    "\n",
    "def paths_to_tensor(img_paths):\n",
    "    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n",
    "    return np.vstack(list_of_tensors)\n",
    "\n",
    "test_tensors = preprocess_input(paths_to_tensor(test_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.load_weights('signlanguage.model.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|███████████████████████████▍            | 138/201 [01:51<00:50,  1.24it/s]\n",
      " 69%|███████████████████████████▍            | 138/201 [03:49<01:44,  1.66s/it]\n",
      "\n",
      "\n",
      " 49%|███████████████████▌                    | 98/201 [00:17<00:00, 970.26it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 21s 3s/step - loss: 0.5632 - accuracy: 0.7500\n",
      "\n",
      "Testing loss: 0.5632\n",
      "Testing accuracy: 0.7500\n"
     ]
    }
   ],
   "source": [
    "print('\\nTesting loss: {:.4f}\\nTesting accuracy: {:.4f}'.format(*new_model.evaluate(test_tensors, test_targets)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATIAAAETCAYAAAC1NopWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAc4ElEQVR4nO3deZwdVZ338c+3O+nskISQ2IABgoEIKBEjILIJ4gRcEh1RETXwoCyOooPOiMvjgo4yz4yjjiISccmIoEHBhIAQnmgGcEEChDVAGJYQCYSEANmT7v7NH1UNl9B9763kLlXd3zevevWt5Z76NSE/zjl1zilFBGZmRdbS7ADMzHaUE5mZFZ4TmZkVnhOZmRWeE5mZFZ4TmZkVnhNZHyNpiKSrJT0n6YodKOcUSfNrGVuzSDpS0gPNjsPqRx5H1hySPgCcC0wC1gKLgX+JiJt3sNwPAZ8ADo+Ijh0ONOckBTAxIh5qdizWPK6RNYGkc4HvAN8AxgHjgR8A02pQ/J7Ag/0hiVVD0oBmx2ANEBHeGrgBOwPrgJPKXDOIJNE9kW7fAQal544BlgOfBlYCK4DT0nNfBbYAW9N7nA58Bbi0pOy9gAAGpPunAg+T1AofAU4pOX5zyfcOB24Fnkt/Hl5ybiHwNeCPaTnzgTG9/G7d8f9zSfzTgROBB4FngM+XXH8I8Gfg2fTa7wNt6bkb099lffr7vq+k/M8CTwI/7z6Wfmef9B4Hp/u7AauAY5r934a3Hfh71ewA+tsGTAU6uhNJL9ecD/wFGAvsCvwJ+Fp67pj0++cDA9MEsAEYlZ7fNnH1msiAYcDzwH7puXbggPTzC4kMGA2sAT6Ufu/kdH+X9PxC4H+AfYEh6f4Fvfxu3fF/KY3/o8DTwGXACOAAYBMwIb3+9cBh6X33ApYAnyopL4BX9VD+v5L8D2FIaSJLr/loWs5Q4Hrg35v934W3HdvctGy8XYBVUb7pdwpwfkSsjIinSWpaHyo5vzU9vzUiriWpjey3nfF0AQdKGhIRKyLi3h6ueRuwNCJ+HhEdEXE5cD/wjpJrfhoRD0bERmA2MLnMPbeS9AduBX4JjAG+GxFr0/vfC7wWICJui4i/pPd9FLgYOLqK3+nLEbE5jeclIuJHwFLgFpLk/YUK5VnOOZE13mpgTIW+m92Ax0r2H0uPvVDGNolwAzA8ayARsZ6kOXYWsELSNZImVRFPd0y7l+w/mSGe1RHRmX7uTjRPlZzf2P19SftKmifpSUnPk/QrjilTNsDTEbGpwjU/Ag4EvhcRmytcaznnRNZ4fyZpOk0vc80TJJ323canx7bHepImVLdXlJ6MiOsj4niSmsn9JH/BK8XTHdPftjOmLC4iiWtiROwEfB5Qhe+UfRQvaThJv+OPga9IGl2LQK15nMgaLCKeI+kfulDSdElDJQ2UdIKk/5dedjnwRUm7ShqTXn/pdt5yMXCUpPGSdgY+131C0jhJ75Q0DNhM0kTt7KGMa4F9JX1A0gBJ7wP2B+ZtZ0xZjCDpx1uX1hbP3ub8U8CEjGV+F7gtIj4CXAP8cIejtKZyImuCiPgPkjFkXyTp6H4c+Djw2/SSrwOLgLuAu4Hb02Pbc68bgF+lZd3GS5NPC8nTzydInuQdDXyshzJWA29Pr11N8sTx7RGxantiyugzwAdInob+iOR3KfUVYJakZyW9t1JhkqaRPHA5Kz10LnCwpFNqFrE1nAfEmlnhuUZmZoXnRGZmhedEZmaF50RmZoXnRGZmhZerlQE0YEiobUSzw7AMXvfq8c0OwTJ47LFHWbVqVaUBxWW17rRnRMfLZn71KDY+fX1ETO3tvKR/BD5CMoj5buA0kgHcvyKZW/so8N6IWFPuPvlKZG0jGLRfxaFAliN/vOX7zQ7BMnjToVN2uIzo2MSgSe+v6tpNd3yv1+lkknYHzgH2j4iNkmYD7ycZbL0gIi6QdB5wHslqJr1y09LMshEgVbdVNgAYks49HkoyOHsaMCs9P4vy0/kAJzIz2x5qqW5LFkhYVLKd0V1ERPwN+HdgGclac89FxHxgXESsSK9ZQbKcVVm5alqaWUFUV9uCZMmqHtuzkkaR1L72Jlk48wpJH9yecJzIzCwjdde2dtRbgEfSNfeQdCXJSsRPSWqPiBWS2klWEi7LTUszy0ZAS2t1W3nLgMPSFWAEHEeycu9cYEZ6zQxgTqWCXCMzs4yq7sgvKyJukfRrktVdOoA7gJkki2rOlnQ6SbI7qVJZTmRmll1tmpZExJeBL29zeDNJ7axqTmRmll0NamS15ERmZhnVrLO/ZpzIzCyb7gGxOeJEZmYZCVrylTryFY2ZFUOLa2RmVmTCfWRm1ge4j8zMis1PLc2sL6g8/aihnMjMLJvq1xprGCcyM8vOTUszKzzXyMys2NzZb2Z9gWtkZlZo8hQlM+sLXCMzs8JzH5mZFZ5rZGZWaPJTSzPrC1wjM7MiE9DS4hqZmRWZ0i1HnMjMLCMhNy3NrOicyMys8JzIzKzYBMrZy0fy9ejBzHJPaR9ZNVvZcqT9JC0u2Z6X9ClJoyXdIGlp+nNUpZicyMwss1oksoh4ICImR8Rk4PXABuAq4DxgQURMBBak+2U5kZlZZrVIZNs4DvifiHgMmAbMSo/PAqZX+rL7yMwsszp09r8fuDz9PC4iVgBExApJYyt92TUyM8tGGTYYI2lRyXbGy4qT2oB3Aldsb0iukZlZJkJZpiitiogpFa45Abg9Ip5K95+S1J7WxtqBlZVu4hqZmWVW4z6yk3mxWQkwF5iRfp4BzKlUgBOZmWVXfdOyfDHSUOB44MqSwxcAx0tamp67oFI5blqaWTaqXWd/RGwAdtnm2GqSp5hVcyIzs8w8RcnMCi1jZ39DOJGZWXb5qpC5s7+ROlYuZvP9l7H5/svZ8uh8oquDzmcfYvP9l7Fp8YV0baj4lNka6MyP/B/G7zaW108+8IVjd915J0cf8UamTH4Nfz/9HTz//PNNjLBJVJeR/TvEiaxBYss6OlfdRdu+72XQpJOBLjrXLEWDRzNwrxPQsN2aHaJt40MzTmXOvOtecuzsMz/C179xAYsW3807p72Lb3/r35oUXXM5kfVjEQFdHUR0QVcHGjiMlsGjaRlccXK/NcERRx7F6NGjX3Js6YMPcMSRRwFw7FuO57dX/aYZoTWdE1k/pbbhDBg7mc33zWLzPT+F1jZadxrf7LAso/0POJB5V88F4MpfX8Hyxx9vckRNUqNxZLVS10QmaaqkByQ9JKniUhx9WXRsouu5Rxi0/4cZdOCp0NlB5zMPNDssy+jiH/2Eiy+6kMMPeT3r1q2lra2t2SE1nJQ8taxma5S6PbWU1ApcSDIydzlwq6S5EXFfve6ZZ13rlqO2ndCAIQC0jpxA1/onaR29X5Mjsyz2mzSJeb+bD8DSBx/kd9de0+SImiNv48jqmTIPAR6KiIcjYgvwS5J1hvolDRxO14Ynia6tRASda5cj940VzsqVyZPlrq4uLvjG1/noGWc1OaLmyFsfWT3Hke0OlHYgLAcO3faidFmPZGmPgcPrGE5ztQx7BS0778OWB2aDWtCQMbTucgCdzz7M1r/dCB0b2fLwPFqGjKFtn3c2O1wDPvzBk7npvxeyatUq9tlrD/7vl77KunXruPiHFwIwbfq7+fCppzU5yibJV4Wsromsp181XnYgYiYwE6Bl6NiXne9LBrYfCu0vzeWtIyfQOnJCkyKycv7r0st7PP7xcz7Z4EjyJ29Ny3omsuXAK0v29wCeqOP9zKwRajhpvFbqmchuBSZK2hv4G8lSth+o4/3MrAGSuZb9JJFFRIekjwPXA63ATyLi3nrdz8waJ2cVsvpOGo+Ia4Fr63kPM2u8/tS0NLO+SP2sRmZmfY+g//SRmVnf5URmZsXmpqWZFZ1wZ7+ZFV5j51FWw4nMzDLLWR5zIjOz7FwjM7NCk/zU0sz6gJxVyLxmv5llV6uFFSWNlPRrSfdLWiLpjZJGS7pB0tL0Z8UVSJ3IzCwzqbqtCt8FrouIScBBwBLgPGBBREwEFqT7ZTmRmVk2NXpBr6SdgKOAHwNExJaIeJZkSfxZ6WWzgOmVQnIiM7NMkgGxNamRTQCeBn4q6Q5Jl0gaBoyLiBUA6c+xlQpyIjOzjJKFFavZgDGSFpVsZ5QUNAA4GLgoIl4HrKeKZmRP/NTSzDLLMI5sVURM6eXccmB5RNyS7v+aJJE9Jak9IlZIagdWVrqJa2Rmlk2VzcpKuS4ingQel9T9ctfjgPuAucCM9NgMYE6lkFwjM7NMajxp/BPALyS1AQ8Dp5FUsGZLOh1YBpxUqRAnMjPLrFaJLCIWAz01PY/LUo4TmZll5ilKZlZsXljRzIpOXo/MzPqCnOUxJzIzy64lZ5nMiczMMstZHnMiM7NsJGgtylNLSd8DorfzEXFOXSIys9wrUmf/ooZFYWaFkrM81nsii4hZpfuShkXE+vqHZGZ5JpIhGHlScdJ4uvTsfSQrNyLpIEk/qHtkZpZbLapua1g8VVzzHeDvgNUAEXEnyaqOZtYfVbk6bCP70ap6ahkRj28TVGd9wjGzvBMFempZ4nFJhwORLrVxDmkz08z6p7x19lfTtDwL+Adgd+BvwOR038z6qcI1LSNiFXBKA2IxswLI8Kq3hqnmqeUESVdLelrSSklzJE1oRHBmlk8tUlVbw+Kp4prLgNlAO7AbcAVweT2DMrN8K2IiU0T8PCI60u1SykxdMrO+TeRvHFm5uZaj049/kHQe8EuSBPY+4JoGxGZmedTgjvxqlOvsv40kcXVHfGbJuQC+Vq+gzCzfcpbHys613LuRgZhZcRSpRvYCSQcC+wODu49FxH/VKygzy6/uPrI8qZjIJH0ZOIYkkV0LnADcDDiRmfVTeVvqupqnlu8heVnmkxFxGnAQMKiuUZlZbkn5G35RTdNyY0R0SeqQtBOwEvCAWLN+LGcVsqoS2SJJI4EfkTzJXAf8ta5RmVmu1aqzX9KjwFqSFXU6ImJKOvTrV8BewKPAeyNiTblyKjYtI+JjEfFsRPwQOB6YkTYxzayf6p5vWWmr0psjYnJETEn3zwMWRMREYEG6X1a5AbEHlzsXEbdXHaaZ9Rmi7v1f00geMALMAhYCny33hXJNy2+VORfAsRkCq8qrX7UHl8/9Zq2LtToaf+bsZodgGax5rGwLrTqCltqNvwhgvqQALo6ImcC4iFgBEBErJI2tVEi5AbFvrlWkZta3VDPcITVGUukb2WamyarbmyLiiTRZ3SDp/u2Jxy/oNbNMRKbO/lUlfV8vExFPpD9XSroKOAR4SlJ7WhtrJxkpUVaGxGpmlqjF6heShkka0f0ZeCtwDzAXmJFeNgOYUyke18jMLLMadZGNA65Ka3cDgMsi4jpJtwKzJZ0OLANOqlRQNVOURLLU9YSIOF/SeOAVEeGxZGb9kFSbtyhFxMMkM4W2Pb6aZDZR1appWv4AeCNwcrq/Frgwy03MrG+p8TiyHVZN0/LQiDhY0h0AEbEmfS2cmfVDyeoX+ZqjVE0i2yqplXR5a0m7Al11jcrMci1vTwmriec/gauAsZL+hWQJn2/UNSozy7XCNS0j4heSbiPpfBMwPSL8pnGzfkoNXqKnGtU8tRwPbACuLj0WEcvqGZiZ5VdrztqW1fSRXcOLLyEZDOwNPAAcUMe4zCynCtnZHxGvKd1PV8U4s5fLzawfyFkeyz6yPyJul/SGegRjZgXQ4JfvVqOaPrJzS3ZbgIOBp+sWkZnlnshXJqumRjai5HMHSZ/Zb+oTjpnlnYABRersTwfCDo+If2pQPGZWAIV5Qa+kARHRUW7JazPrf4r2gt6/kvSHLZY0F7gCWN99MiKurHNsZpZHDR61X41q+shGA6tJ1ujvHk8WgBOZWT9VpHFkY9MnlvfwYgLrFnWNysxyq2hNy1ZgOPT4nNWJzKzfEq0FqpGtiIjzGxaJmRVC8vKRZkfxUuUSWc5CNbNcKNjI/kxrZptZ/1GYzv6IeKaRgZhZMRStaWlm1qPC1MjMzHoioDVfecyJzMwyUoHmWpqZ9SZfacyJzMwyyuNS1zlbVcjMikBVblWVJbVKukPSvHR/tKQbJC1Nf46qVIYTmZllJFpaqtuq9Emg9BWT5wELImIisCDdL8uJzMwyEUniqGarWJa0B/A24JKSw9OAWennWcD0SuW4j8zMMqvhU8vvAP/MS5fUHxcRKwAiYoWksZUKcY3MzDLL0Ec2RtKiku2MF8qQ3g6sjIjbdjQe18jMLJts48hWRcSUXs69CXinpBNJXv69k6RLgacktae1sXZgZaWbuEZmZpnUqo8sIj4XEXtExF7A+4HfR8QHgbnAjPSyGcCcSjG5RmZmmdV5HNkFwGxJpwPLgJMqfcGJzMwyq3Uei4iFwML082oyLiPmRGZmmSRNy3yN7HciM7PMcjZDyYnMzLISco3MzIrONTIzKzSJQr0OzsysRznLY05kjbJ50yZOO2kqW7dsoaOjg+NPnMbHPv0F5s+7iou+/U0eeegBfjH3Dxxw0MHNDtVSXZvXs/aPF9O55nEARhx5NmptY+2fLiE6NtE6fFdGHP0JWtqGNjnSxnMfWT/VNmgQl/xyHkOHDWfr1q2c+vdv5Yg3H8+r9tufb8/8BV/73CebHaJtY90tP6Nt94MYcuy5RGcH0bGZ567/OsPe8CHa2vdn44N/YOPdVzPs9e9rdqgNlSys2OwoXspTlBpEEkOHDQego2MrHR0dIDFh4n7stc/EJkdn2+rasoGtTy5h8L7HAqDWAbQMGkbncysY+IpXA9C222vY/NgtzQyzaVTlP43iGlkDdXZ2cvLbjmLZow/zvg9/lNe+7g3NDsl60bV2JS2Dd2LtTRfR+cxjDBizN8MPPZXWUa9ky7JFDNrzDWx+9C90rVvd7FCbot8sdS3pJ5JWSrqnXvcomtbWVmZf90fm37KEe+68jaUP3NfskKwXEZ10rH6EIZOOZ9T0f0UDBrPhrjmMOOIsNi6Zz5o55xFbN0Jr/6sLdDctq9kapZ5Ny58BU+tYfmHttPNI3nDYEfxp4f9vdijWi9ahu9AybBcGjk2a/W17HUrH6kcYMHJ3Rk79AqOmXcCgCW+idcS4JkfaDNU2LBuXyeqWyCLiRuCZepVfNM+sXsXzzz0LwKZNG/nLzQvdN5ZjLUNH0jJsFzqeewKArU/cQ+vIPeja+BwAEV1sWHwlgycd38wwm0PJ8Itqtkbpf/XiJlm18km+eO5ZdHV20tXVxVvf/i6OfssJLLjuai740j+x5plVfPy0k9hv/9fww0t/2+xwDRhx2GmsXfg9oquD1hFjGXHk2Wx66EY2LZkPQNuehzB44jHNDbJJ8tVDloNEli59ewZA++6vbHI09bPvqw9k9u9uftnx46a+g+OmvqMJEVklA3bZi1HTvvmSY0MPOJGhB5zYpIjywe+17EFEzIyIKRExZdToMc0Ox8yq4KalmRVe3kb213P4xeXAn4H9JC1Pl601sz6g39TIIuLkepVtZs2Vr/qYm5Zmtj1ylsmcyMwsk+Tlu/nKZE5kZpZNg6cfVcOJzMyycyIzs2Lzy0fMrA/I2cB+JzIzy0bkrmXZ/ClKZlY8kqraKpQxWNJfJd0p6V5JX02Pj5Z0g6Sl6c9RleJxIjOzzGo0sn8zcGxEHARMBqZKOgw4D1gQEROBBel+WU5kZpaZqtzKicS6dHdgugUwDZiVHp8FTK8UjxOZmWVTbRaroiNNUqukxcBK4IaIuAUYFxErANKfYyuV485+M8ssw/CLMZIWlezPjIiZ3TsR0QlMljQSuErSgdsTjxOZmWUiMg2/WBURUypdFBHPSlpI8p6PpyS1R8QKSe0ktbWy3LQ0s8xq0dkvade0JoakIcBbgPuBucCM9LIZwJxK8bhGZmaZ1WhkfzswS1IrSaVqdkTMk/RnYHa6huEy4KRKBTmRmVlmtRjZHxF3Aa/r4fhq4LgsZTmRmVlmeRvZ70RmZtnlLJM5kZlZJlL+XgfnRGZmmeUrjTmRmdn2yFkmcyIzs4y8sKKZ9QE56yJzIjOzbPK4sKITmZllVmnRxEZzIjOzzHKWx5zIzCy7nOUxJzIzy6i6ZawbyonMzLZDvjKZE5mZZZJxYcWGcCIzs8xanMjMrOg8st/Mii9fecyJzMyyy1kecyIzs2yqfIt4QzmRmVlmnqJkZoWXrzTmRGZm2yFnFTInMjPLygsrmlnB5XFkf0uzAzAz21GukZlZZnl7HZxrZGaWjV4cS1ZpK1uM9EpJf5C0RNK9kj6ZHh8t6QZJS9OfoyqF5ERmZpkow1ZBB/DpiHg1cBjwD5L2B84DFkTERGBBul+WE5mZZVeDTBYRKyLi9vTzWmAJsDswDZiVXjYLmF4pHPeRmVlmtR5+IWkv4HXALcC4iFgBSbKTNLbS953IzCyzDH39YyQtKtmfGREzX1qWhgO/AT4VEc9vz/QnJzIzyyxDrlkVEVN6L0cDSZLYLyLiyvTwU5La09pYO7Cy0k3cR2ZmmanKf8qWkVS9fgwsiYj/KDk1F5iRfp4BzKkYT0Rs569Se5KeBh5rdhx1MAZY1ewgLJO++me2Z0TsuiMFSLqO5N9PNVZFxNReyjkCuAm4G+hKD3+epJ9sNjAeWAacFBHPlI0pT4msr5K0qFz12vLHf2bF4qalmRWeE5mZFZ4TWWPMrHyJ5Yz/zArEfWRmVniukZlZ4TmRmVnhOZGZWeF5ilIdSJpEMoN/dyCAJ4C5EbGkqYGZ9VGukdWYpM8CvyRZxOSvwK3p58slVVxXyfJF0mnNjsEq81PLGpP0IHBARGzd5ngbcG+6WJwVhKRlETG+2XFYeW5a1l4XsBsvnzPazovzySxHJN3V2ylgXCNjse3jRFZ7nwIWSFoKPJ4eGw+8Cvh406KycsYBfwes2ea4gD81PhzLyomsxiLiOkn7AoeQdPYLWA7cGhGdTQ3OejMPGB4Ri7c9IWlh48OxrNxHZmaF56eWZlZ4TmRmVnhOZAUiqVPSYkn3SLpC0tAdKOtnkt6Tfr4kfZ9gb9ceI+nw7bjHo5JetpJob8e3uWZdxnt9RdJnssZofYMTWbFsjIjJEXEgsAU4q/SkpNbtKTQiPhIR95W55BggcyIzaxQnsuK6CXhVWlv6g6TLgLsltUr6N0m3SrpL0pmQvOhB0vcl3SfpGuCFdwVKWihpSvp5qqTbJd0paUH6vsGzgH9Ma4NHStpV0m/Se9wq6U3pd3eRNF/SHZIupoqXTUv6raTbJN0r6Yxtzn0rjWWBpF3TY/tIui79zk3pdDDr7yLCW0E2YF36cwDJm2XOJqktrQf2Ts+dAXwx/TwIWATsDbwbuAFoJRmw+yzwnvS6hcAUYFeSsW/dZY1Of34F+ExJHJcBR6Sfx5O8BQfgP4EvpZ/fRjLPdEwPv8ej3cdL7jEEuAfYJd0P4JT085eA76efFwAT08+HAr/vKUZv/WvzOLJiGSKpe6zTTSSv0joc+GtEPJIefyvw2u7+L2BnYCJwFHB5JGPZnpD0+x7KPwy4sbus6P3NNW8B9i95kepOkkak93h3+t1rJG07wLQn50h6V/r5lWmsq0lmQfwqPX4pcGX6ItfDgStK7j2ointYH+dEViwbI2Jy6YH0L/T60kPAJyLi+m2uO5GkllOOqrgGki6JN0bExh5iqXpgoqRjSJLiGyNiQzr4dHAvl0d632e3/Xdg5j6yvud64Oz0Dc5I2lfSMOBG4P1pH1o78OYevvtn4GhJe6ffHZ0eXwuMKLluPiXTrSR1J5YbgVPSYycAoyrEujOwJk1ik0hqhN1agO5a5QeAmyPieeARSSel95Ckgyrcw/oBJ7K+5xLgPuB2SfcAF5PUvK8ClpK8DPUi4L+3/WJEPE3Sx3alpDt5sWl3NfCu7s5+4BxgSvow4T5efHr6VeAoSbeTNHGXVYj1OmBAOmn7a8BfSs6tBw6QdBtwLHB+evwU4PQ0vntJ1n2zfs5TlMys8FwjM7PCcyIzs8JzIjOzwnMiM7PCcyIzs8JzIjOzwnMiM7PCcyIzs8L7X+G5xjYJsquXAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm_labels = ['0','1']\n",
    "\n",
    "cm = confusion_matrix(np.argmax(test_targets, axis=1),\n",
    "                      np.argmax(new_model.predict(test_tensors), axis=1))\n",
    "plt.imshow(cm, cmap=plt.cm.Blues)\n",
    "plt.colorbar()\n",
    "indexes = np.arange(len(cm_labels))\n",
    "for i in indexes:\n",
    "    for j in indexes:\n",
    "        plt.text(j, i, cm[i, j])\n",
    "plt.xticks(indexes, cm_labels, rotation=90)\n",
    "plt.xlabel('Predicted label')\n",
    "plt.yticks(indexes, cm_labels)\n",
    "plt.ylabel('True label')\n",
    "plt.title('Confusion matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through its layers and lock them to make them not trainable with this code\n",
    "for layer in base_model.layers[:-4]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
